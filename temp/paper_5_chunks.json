{
  "paper_id": 4,
  "filename": "paper_5.pdf",
  "metadata": {
    "title": "253 255..260",
    "authors": "i n f o r m e dd i s c u s s i o n sa m o n gp o l i c y - m a k e r sa n dt h e, public about data and the capabilities of machinelearning, will lead to insightful designs of programs, and policies that can balance the goals of protecting, privacy and ensuring fairness with those of reapingthe benefits to scientific research and to individual, and public health. Our commitments to privacy and",
    "year": "2014",
    "pages": 7
  },
  "chunks": [
    {
      "text": "Despite practical challe nges, we are hopeful that i n f o r m e dd i s c u s s i o n sa m o n gp o l i c y - m a k e r sa n dt h e public about data and the capabilities of machinelearning, will lead to insightful designs of programs and policies that can balance the goals of protecting privacy and ensuring fairness with those of reapingthe benefits to scientific research and to individual and public health. Our commitments to privacy and fairness are evergreen, but our policy choices mustadapt to advance them, and support new tech-niques for deepening our knowledge.",
      "section": "Unknown",
      "page_start": 1,
      "page_end": 1,
      "chunk_index": 0
    },
    {
      "text": "fairness are evergreen, but our policy choices mustadapt to advance them, and support new tech-niques for deepening our knowledge. 1. M. De Choudhury, S. Counts, E. Horvitz, A. Hoff, in Proceedings of International Conference on Weblogs and Social Media [Association for the Advancement of Artificial Intelligence (AAAI), Palo Alto, CA, 2014]. 2. J. S. Brownstein, C. C. Freifeld, L. C. Madoff, N. Engl. J. Med. 360, 2153 –2155 (2009). 3. G. Eysenbach, J. Med. Internet Res. 11, e11 (2009). 4. D. A. Broniatowski, M. J. Paul, M. Dredze, PLOS ONE 8, e83672 (2013). 5. A. Sadilek, H. Kautz, V. Silenzio, in Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI, Palo Alto, CA, 2012). 6. M. De Choudhury, S. Counts, E. Horvitz, in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Association for Computing Machinery, New York, 2013), pp. 3267 –3276. 7. R. W. White, R. Harpaz, N. H. Shah, W. DuMouchel, E. Horvitz, Clin. Pharmacol.",
      "section": "Unknown",
      "page_start": 1,
      "page_end": 1,
      "chunk_index": 1
    },
    {
      "text": "pp. 3267 –3276. 7. R. W. White, R. Harpaz, N. H. Shah, W. DuMouchel, E. Horvitz, Clin. Pharmacol. Ther. 96, 239 –246 (2014). 8. Samaritans Radar; www.samaritans.org/how-we-can-help-you/ supporting-someone-online/samaritans-radar. 9. Shut down Samaritans Radar; http://bit.ly/Samaritans-after. 10. U.S. Equal Employment Opportunity Commission (EEOC), 29 Code of Federal Regulations (C.F.R.), 1630.2 (g) (2013). 11. EEOC, 29 CFR 1635.3 (c) (2013). 12. M. A. Rothstein, J. Law Med. Ethics 36, 837 –840 (2008). 13. Executive Office of the President, Big Data: Seizing Opportunities, Preserving Values (White House, Washington, DC, 2014); http://1.usa.gov/1TSOhiG. 14. Letter from Maneesha Mithal, FTC, to Reed Freeman, Morrison, & Foerster LLP, Counsel for Netflix, 2 [closing letter] (2010); http://1.usa.gov/1GCFyXR. 15. In re Facebook, Complaint, FTC File No. 092 3184 (2012).16. FTC Staff Report, Mobile Privacy Disclosures: Building Trust Through",
      "section": "References",
      "page_start": 1,
      "page_end": 1,
      "chunk_index": 2
    },
    {
      "text": "15. In re Facebook, Complaint, FTC File No. 092 3184 (2012).16. FTC Staff Report, Mobile Privacy Disclosures: Building Trust Through Transparency ( F T C ,W a s h i n g t o n ,D C ,2 0 1 3 ) ;h t t p : / / 1 . u s a . g o v / 1 e N z 8 z r . 17. FTC, Protecting Consumer Privacy in an Era of Rapid Change: Recommendations for Businesses and Policymakers (FTC, Washington, DC, 2012). 18. Directive 95/46/ec of the European Parliament and of The Council of Europe, 24 October 1995. 19. L. Sweeney, Online ads roll the dice [blog]; http://1.usa.gov/ 1KgEcYg. 20. FTC, “Big data: A tool for inclusion or exclusion? ”(workshop, FTC, Washington, DC, 2014); http://1.usa.gov/1SR65cv 21. FTC, Data Brokers: A Call for Transparency and Accountability (FTC, Washington, DC, 2014); http://1.usa.gov/1GCFoj5. 22. J. Podesta, “Big data and privacy: 1 year out ”[blog]; http://bit. ly/WHsePrivacy. 23. White House Council of Economic Advisers, Big Data and",
      "section": "References",
      "page_start": 1,
      "page_end": 1,
      "chunk_index": 3
    },
    {
      "text": "22. J. Podesta, “Big data and privacy: 1 year out ”[blog]; http://bit. ly/WHsePrivacy. 23. White House Council of Economic Advisers, Big Data and Differential Pricing (White House, Washington, DC, 2015). 24. Executive Office of the President, Big Data and Differential Processing (White House, Washington, DC, 2015 ); http://1.usa.gov/1eNy7qR. 25. Executive Office of the President, Big Data: Seizing Opportunities, Preserving Values (White House, Washington, DC, 2014); http://1.usa.gov/1TSOhiG. 26. President ’s Council of Advisors on Science and Technology (PCAST), Big Data and Privacy: A Technological Perspective (White House, Washington, DC, 2014); http://1.usa.gov/1C5ewNv. 27. European Commission, Proposal for a Regulation of the European Parliament and of the Council on the Protection of Individualswith regard to the processing of personal data and on the free movement of such data (General Data Protection Regulation),COM(2012) 11 final (2012); http://bit.ly/1Lu5POv. 28.M.",
      "section": "References",
      "page_start": 1,
      "page_end": 1,
      "chunk_index": 4
    },
    {
      "text": "movement of such data (General Data Protection Regulation),COM(2012) 11 final (2012); http://bit.ly/1Lu5POv. 28.M. Schrems v.Facebook Ireland Limited , §J. Unlawful data transmission to the U.S.A. ( “PRISM ”), ¶166 and 167 (2013); www.europe-v-facebook.org/sk/sk_en.pdf. 10.1126/science.aac4520REVIEW Machine learning: Trends, perspectives, and prospects M. I. Jordan1*and T. M. Mitchell2* Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today ’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in theavailability of online data and low-cost computation.",
      "section": "References",
      "page_start": 1,
      "page_end": 1,
      "chunk_index": 5
    },
    {
      "text": "the development of new learning algorithms and theory and by the ongoing explosion in theavailability of online data and low-cost computation. The adoption of data-intensivemachine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing. Machine learning is a discipline focused on two interrelated questions: How canone construct computer systems that auto-matically improve through experience? and What are the fundamental statistical- computational-information-theoretic laws that govern all learning systems, including computers, humans, and organizations? The study of machine learning is important both for addressing thesefundamental scientific and engineering ques-tions and for the highly practical computer soft- w a r ei th a sp r o d u c e da n df i e l d e da c r o s sm a n y applications.",
      "section": "References",
      "page_start": 1,
      "page_end": 1,
      "chunk_index": 6
    },
    {
      "text": "w a r ei th a sp r o d u c e da n df i e l d e da c r o s sm a n y applications. Machine learning has progressed dramati- cally over the past two decades, from laboratory curiosity to a practical technology in widespread commercial use. Within artificial intelligence (AI),machine learning has emerged as the method of choice for developing practical software for computer vision, speech recognition, natural lan-guage processing, robot control, and other ap-plications. Many developers of AI systems now recognize that, for many applications, it can be far easier to train a system by showing it exam-ples of desired input-output behavior than toprogram it manually by anticipating the desired response for all possible inputs. The effect of ma- chine learning has also been felt broadly acrosscomputer science and across a range of indus-tries concerned with data-intensive issues, such as consumer services, the diagnosis of faults in complex systems, and the control of logisticschains.",
      "section": "References",
      "page_start": 1,
      "page_end": 1,
      "chunk_index": 7
    },
    {
      "text": "as consumer services, the diagnosis of faults in complex systems, and the control of logisticschains. There has been a similarly broad range of effects across empirical sciences, from biology to cosmology to social science, as machine-learningmethods have been developed to analyze high-throughput experimental data in novel ways. See Fig. 1 for a depiction of some recent areas of ap-plication of machine learning. A learning problem can be defined as the problem of improving some measure of perform-ance when executing some task, through sometype of training experience. For example, in learn-ing to detect credit-card fraud, the task is to as-sign a label of “fraud ”or“not fraud” to any given credit-card transaction. The performance metricto be improved might be the accuracy of thisfraud classifier, and the training experience might consist of a collection of historical credit-card transactions, each labeled in retrospect as fraud-ulent or not.",
      "section": "References",
      "page_start": 1,
      "page_end": 1,
      "chunk_index": 8
    },
    {
      "text": "consist of a collection of historical credit-card transactions, each labeled in retrospect as fraud-ulent or not. Alternatively, one might define adifferent performance metric that assigns a higher penalty when “fraud ”is labeled “not fraud ”than when “not fraud ”is incorrectly labeled “fraud. ” One might also define a dif ferent type of training experience —for example, by including unlab- eled credit-card transac tions along with labeled examples. A diverse array of machine-learning algorithms has been developed to cover the wide variety of data and problem types exh ibited across differ- ent machine-learning problems (1 ,2). Conceptual- ly, machine-learning algorithms can be viewed as searching through a large space of candidate programs, guided by training experience, to finda program that optimizes the performance metric.Machine-learning algorithms vary greatly, in part by the way in which they represent candidate",
      "section": "References",
      "page_start": 1,
      "page_end": 1,
      "chunk_index": 9
    },
    {
      "text": "by the way in which they represent candidate programs (e.g., decision trees, mathematical func-tions, and general programming languages) and inp a r tb yt h ew a yi nw h i c ht h e ys e a r c ht h r o u g ht h i s space of programs (e.g., optimization algorithms with well-understood c onvergence guarantees and evolutionary search methods that evaluate s u c c e s s i v eg e n e r a t i o n so fr a n d o m l ym u t a t e dp r o - grams). Here, we focus on approaches that havebeen particularly successful to date. Many algorithms focus on function approxi- mation problems, where the task is embodiedin a function (e.g., given an input transaction, out-put a “fraud ”or“not fraud ”label), and the learn- ing problem is to improve the accuracy of that function, with experience consisting of a sample of known input-output pairs of the function. Insome cases, the function is represented explicit- ly as a parameterized functional form; in other",
      "section": "References",
      "page_start": 1,
      "page_end": 1,
      "chunk_index": 10
    },
    {
      "text": "of known input-output pairs of the function. Insome cases, the function is represented explicit- ly as a parameterized functional form; in other cases, the function is implicit and obtained via asearch process, a factorization, an optimization SCIENCE sciencemag.org 17 JULY 2015 VOL 349 ISSUE 6245 255 1Department of Electrical Engineering and Computer Sciences, Department of Statistics, University of California, Berkeley, CA, USA.2Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA. *Corresponding author. E-mail: jordan@cs.berkeley.edu (M.I.J.); tom.mitchell@cs.cmu.edu (T.M.M.) on July 20, 2015 www.sciencemag.org Downloaded from  on July 20, 2015 www.sciencemag.org Downloaded from  on July 20, 2015 www.sciencemag.org Downloaded from  on July 20, 2015 www.sciencemag.org Downloaded from  on July 20, 2015 www.sciencemag.org Downloaded from  on July 20, 2015 www.sciencemag.org Downloaded from procedure, or a simulation-based procedure. Even",
      "section": "References",
      "page_start": 1,
      "page_end": 2,
      "chunk_index": 11
    },
    {
      "text": "procedure, or a simulation-based procedure. Even when implicit, the function generally dependson parameters or other tunable degrees of free-d o m ,a n dt r a i n i n gc o r r e s p o n d st of i n d i n gv a l u e s for these parameters that optimize the perform- ance metric. Whatever the learning algorithm, a key scien- tific and practical goal is to theoretically character- ize the capabilities of specific learning algorithmsand the inherent difficulty of any given learningproblem: How accurately can the algorithm learn from a particular type and volume of training data? How robust is the algorithm to errors in itsmodeling assumptions or to errors in the train-ing data? Given a learning problem with a given volume of training data, is it possible to design a successful algorithm or is this learning problemfundamentally intractable? Such theoretical char-acterizations of machine-learning algorithms and problems typically make use of the familiar frame-",
      "section": "References",
      "page_start": 2,
      "page_end": 2,
      "chunk_index": 12
    },
    {
      "text": "Such theoretical char-acterizations of machine-learning algorithms and problems typically make use of the familiar frame- works of statistical decision theory and compu-tational complexity theory. In fact, attempts to characterize machine-learning algorithms the- oretically have led to blends of statistical andc o m p u t a t i o n a lt h e o r yi nw h i c ht h eg o a li st os i m u l -taneously characterize the sample complexity (how much data are require d to learn accurately) and the computational c omplexity (how much computation is required) and to specify how thesedepend on features of the learning algorithm such as the representation it uses for what it learns (3–6). A specific form of computational analysis that has proved particularly useful in recent years has been that of optimization theory, with upper and lower bounds on rates of con-vergence of optimization procedures mergingwell with the formulation of machine-learning problems as the optimization of a performance",
      "section": "References",
      "page_start": 2,
      "page_end": 2,
      "chunk_index": 13
    },
    {
      "text": "problems as the optimization of a performance metric ( 7,8). As a field of study, machine learning sits at the crossroads of computer science, statistics and a variety of other disciplines concerned with auto- matic improvement over time, and inference and decision-making under uncertainty. Related dis- ciplines include the psychological study of human learning, the study of evolution, adaptive controltheory, the study of educational practices, neuro-science, organizational behavior, and economics. Although the past decade has seen increased cross- talk with these other fields, we are just beginningto tap the potential synergies and the diversityof formalisms and experimental methods used across these multiple fields for studying systems that improve with experience. Drivers of machine-learning progress The past decade has seen rapid growth in the ability of networked and mobile computing sys-tems to gather and transport vast amounts of data, a phenomenon often referred to as “Big",
      "section": "References",
      "page_start": 2,
      "page_end": 2,
      "chunk_index": 14
    },
    {
      "text": "ability of networked and mobile computing sys-tems to gather and transport vast amounts of data, a phenomenon often referred to as “Big Data. ”The scientists and engineers who collect such data have often turned to machine learn-ing for solutions to the problem of obtaining useful insights, predictions, and decisions from such data sets. Indeed, the sheer size of the datamakes it essential to develop scalable proce-dures that blend computational and statistical 256 17 JULY 2015 VOL 349 ISSUE 6245 sciencemag.org SCIENCE sunglasses 0.52antelope 0.68DialogVisualCharacters SemanticsMotionSyntax starfish 0.67milk can 1.00milk can 1.00person 0.92 orange 0.73 bird 0.78 lemon 0.86bird 0.69 bird 0.95isopod 0.55 Fig. 1. Applications of machine learning. Machine learning is having a substantial effect on many",
      "section": "References",
      "page_start": 2,
      "page_end": 2,
      "chunk_index": 15
    },
    {
      "text": "bird 0.78 lemon 0.86bird 0.69 bird 0.95isopod 0.55 Fig. 1. Applications of machine learning. Machine learning is having a substantial effect on many areas of technology and science; examples of recent applied success stories include robotics andautonomous vehicle control (top left), speech processing and natural language processing (topright), neuroscience research (middle), and applications in computer vision (bottom). [The middle panel is adapted from ( 29). The images in the bottom panel are from the ImageNet database; object recognition annotation is by R. Girshick.]CREDIT: ISTOCK/AKINBOSTANCIARTIFICIAL INTELLIGENCE considerations, but the issue is more than the mere size of modern data sets; it is the granular,personalized nature of much of these data. Mo-bile devices and embedded computing permit large amounts of data to be gathered about in- dividual humans, and machine-learning algo-rithms can learn from these data to customize their services to the needs and circumstances",
      "section": "References",
      "page_start": 2,
      "page_end": 3,
      "chunk_index": 16
    },
    {
      "text": "dividual humans, and machine-learning algo-rithms can learn from these data to customize their services to the needs and circumstances of each individual. Moreover, these personalizedservices can be connected, so that an overall ser-vice emerges that takes advantage of the wealth and diversity of data from many individuals while still customizing to the needs and circum-stances of each. Instances of this trend towardcapturing and mining large quantities of data to improve services and productivity can be found across many fields of commerce, science, andgovernment. Historical medical records are usedto discover which patients will respond best to which treatments; historical traffic data are used to improve traffic control and reduce con-gestion; historical crime data are used to help allocate local police to specific locations at spe- cific times; and large experimental data sets arecaptured and curated to accelerate progress inbiology, astronomy, neuroscience, and other data-",
      "section": "References",
      "page_start": 3,
      "page_end": 3,
      "chunk_index": 17
    },
    {
      "text": "cific times; and large experimental data sets arecaptured and curated to accelerate progress inbiology, astronomy, neuroscience, and other data- intensive empirical sciences. We appear to be at the beginning of a decades-long trend toward in-creasingly data-intensive, evidence-based decision-making across many aspects of science, commerce, and government. With the increasing prominence of large-scale data in all areas of human endeavor has come a wave of new demands on the underlying machine- learning algorithms. For example, huge data setsrequire computationally tractable algorithms, high-ly personal data raise the need for algorithms that minimize privacy effects, and the availabil- ity of huge quantities of unlabeled data raisesthe challenge of designing learning algorithmsto take advantage of it. The next sections survey some of the effects of these demands on recentwork in machine-learning algorithms, theory, and practice. Core methods and recent progress",
      "section": "References",
      "page_start": 3,
      "page_end": 3,
      "chunk_index": 18
    },
    {
      "text": "some of the effects of these demands on recentwork in machine-learning algorithms, theory, and practice. Core methods and recent progress The most widely used machine-learning methods are supervised learning methods ( 1). Supervised learning systems, including spam classifiers of e-mail, face recognizers over images, and med- ical diagnosis systems for patients, all exemplifythe function approximation problem discussedearlier, where the training data take the form of a collection of ( x,y) pairs and the goal is to produce a prediction y*i nr e s p o n s et oaq u e r y x*. The inputs xm a yb ec l a s s i c a lv e c t o r so rt h e y m a yb em o r ec o m p l e xo b j e c t ss u c ha sd o c u m e n t s , images, DNA sequences, or graphs. Similarly, many different kinds of output yhave been studied. Much progress has been made by focusing onthe simple binary classification problem in which ytakes on one of two values (for example, “spam ” or“not spam ”), but there has also been abun-",
      "section": "References",
      "page_start": 3,
      "page_end": 3,
      "chunk_index": 19
    },
    {
      "text": "ytakes on one of two values (for example, “spam ” or“not spam ”), but there has also been abun- dant research on problems such as multiclass classification (where ytakes on one of Klabels), multilabel classification (where yis labeled simul- taneously by several of the Klabels), ranking problems (where yprovides a partial order on some set), and general structured prediction problems (where yis a combinatorial object such as a graph, whose components may be requiredto satisfy some set of constraints). An example of the latter problem is part-of-speech tagging, where the goal is to simultaneously label everyword in an input sentence xas being a noun, verb, or some other part of speech. Supervised learning also includes cases in which yhas real- valued components or a mixture of discrete andreal-valued components. Supervised learning systems generally form their predictions via a learned mapping f(x), which produces an output yfor each input x(or",
      "section": "References",
      "page_start": 3,
      "page_end": 3,
      "chunk_index": 20
    },
    {
      "text": "Supervised learning systems generally form their predictions via a learned mapping f(x), which produces an output yfor each input x(or a probability distribution over ygiven x). Many different forms of mapping fexist, includingdecision trees, decision forests, logistic regres- sion, support vector machines, neural networks,kernel machines, and Bayesian classifiers ( 1). A variety of learning algorithms has been proposed to estimate these different types of mappings, and there are also generic procedures such as boost-ing and multiple kernel learning that combine the outputs of multiple learning algorithms. Procedures for learning ffrom data often make use of ideas from optimization theory or numer-ical analysis, with the specific form of machine- learning problems (e.g., that the objective function or function to be integrated is often the sum overa large number of terms) dr iving innovations. This diversity of learning archi tectures and algorithms",
      "section": "References",
      "page_start": 3,
      "page_end": 3,
      "chunk_index": 21
    },
    {
      "text": "This diversity of learning archi tectures and algorithms reflects the diverse needs of applications, with different architectures ca pturing different kinds of mathematical structures, offering different lev-els of amenability to post-hoc visualization and explanation, and providing varying trade-offs between computational complexity, the amountof data, and performance. One high-impact area of progress in supervised learning in recent years involves deep networks,which are multilayer networks of threshold units,each of which computes some simple param- eterized function of its inputs ( 9,10). Deep learning systems make use of gradient-based optimiza-tion algorithms to adjust parameters throughouts u c ham u l t i l a y e r e dn e t w o r kb a s e do ne r r o r sa t its output. Exploiting modern parallel comput- ing architectures, such as graphics processingunits originally develop ed for video gaming, it has been possible to build deep learning sys-",
      "section": "References",
      "page_start": 3,
      "page_end": 3,
      "chunk_index": 22
    },
    {
      "text": "ing architectures, such as graphics processingunits originally develop ed for video gaming, it has been possible to build deep learning sys- tems that contain billions of parameters andthat can be trained on the very large collectionsof images, videos, and speech samples available on the Internet. Such large-scale deep learning systems have had a major effect in recent yearsin computer vision ( 11) and speech recognition (12), where they have yielded major improve- ments in performance over previous approaches SCIENCE sciencemag.org 17 JULY 2015 VOL 349 ISSUE 6245 257 Input image Convolutional feature extraction 14 x 14 feature mapRNN with attention over image Word by word generation AA bird flying over a body of water bird flying over a body of waterLSTM . Fig. 2. Automatic generation of text captions for images with deep networks. A convolutional neural network is trained to interpret images, and its",
      "section": "References",
      "page_start": 3,
      "page_end": 3,
      "chunk_index": 23
    },
    {
      "text": ". Fig. 2. Automatic generation of text captions for images with deep networks. A convolutional neural network is trained to interpret images, and its output is then used by a recurrent neural network trained to generate a text caption (top). The sequence at the bottom shows the word-by-word focus of the network on different parts of input image while it generates the caption word-by-word. [Adapted with permission from ( 30)] CREDIT: ISTOCK/CORR (see Fig. 2). Deep network methods are being actively pursued in a variety of additional appli- cations from natural language translation tocollaborative filtering. The internal layers of deep networks can be viewed as providing learned representations of the input data. While much of the practical suc-cess in deep learning has come from supervisedlearning methods for discovering such repre- sentations, efforts have also been made to devel-",
      "section": "References",
      "page_start": 3,
      "page_end": 4,
      "chunk_index": 24
    },
    {
      "text": "sentations, efforts have also been made to devel- op deep learning algorithms that discover usefulrepresentations of the input without the need forlabeled training data ( 13). The general problem is referred to as unsupervised learning, a second paradigm in machine- learning research ( 2). Broadly, unsupervised learning generally in- volves the analysis of unlabeled data under as- sumptions about structural properties of thedata (e.g., algebraic, combinatorial, or probabi-listic). For example, one can assume that data lie on a low-dimensional manifold and aim to identify that manifold explicitly from data. Di-mension reduction methods —including prin- cipal components analysis, manifold learning, factor analysis, random projections, and autoen- coders ( 1,2)—make different specific assump- tions regarding the underlying manifold (e.g., that it is a linear subspace, a smooth nonlinear manifold, or a collection of submanifolds).",
      "section": "References",
      "page_start": 4,
      "page_end": 4,
      "chunk_index": 25
    },
    {
      "text": "tions regarding the underlying manifold (e.g., that it is a linear subspace, a smooth nonlinear manifold, or a collection of submanifolds). An-other example of dimension reduction is thetopic modeling framework depicted in Fig. 3. A criterion function is defined that embodies these assumptions —often making use of general statistical principles such as maximum like-lihood, the method of moments, or Bayesian integration —and optimization or sampling algo-rithms are developed to optimize the criterion. As another example, clustering is the problem of finding a partition of the observed data (anda rule for predicting future data) in the absenceof explicit labels indicating a desired partition. A wide range of clustering procedures has been developed, all based on specific assumptionsregarding the nature of a “cluster. ”In both clus- tering and dimension reduction, the concern with computational complexity is paramount,",
      "section": "References",
      "page_start": 4,
      "page_end": 4,
      "chunk_index": 26
    },
    {
      "text": "tering and dimension reduction, the concern with computational complexity is paramount, given that the goal is to exploit the particularlylarge data sets that are available if one dis-penses with supervised labels. A third major machine-learning paradigm is reinforcement learning ( 14,15). Here, the infor- mation available in the training data is inter- mediate between supervised and unsupervised learning. Instead of training examples that in-dicate the correct output for a given input, thetraining data in reinforcement learning are as- sumed to provide only an indication as to whether an action is correct or not; if an action is incor-rect, there remains the problem of finding thecorrect action. More generally, in the setting of sequences of inputs, it is assumed that reward signals refer to the entire sequence; the assign-ment of credit or blame to individual actions in the sequence is not directly provided. Indeed, although",
      "section": "References",
      "page_start": 4,
      "page_end": 4,
      "chunk_index": 27
    },
    {
      "text": "sequence is not directly provided. Indeed, although simplified versions of reinforcement learningknown as bandit problems are studied, where itis assumed that rewards are provided after each action, reinforcement learning problems typically involve a general control-theoretic setting inwhich the learning task is to learn a control strat-egy (a “policy ”) for an agent acting in an unknown dynamical environment, where that learned strat-egy is trained to chose actions for any given state, with the objective of maximizing its expected re- ward over time. The ties to research in controltheory and operations research have increasedover the years, with formulations such as Markov decision processes and pa rtially observed Mar- kov decision processes pr oviding points of con- tact ( 15,16). Reinforcement-learning algorithms generally make use of ideas that are familiar from the control-theory literature, such as policy iteration, value iteration , rollouts, and variance",
      "section": "References",
      "page_start": 4,
      "page_end": 4,
      "chunk_index": 28
    },
    {
      "text": "generally make use of ideas that are familiar from the control-theory literature, such as policy iteration, value iteration , rollouts, and variance reduction, with innovations arising to addressthe specific needs of machine learning (e.g., large- scale problems, few assumptions about the un- known dynamical environment, and the use ofsupervised learning architectures to represent policies). It is also worth noting the strong ties between reinforcement l earning and many dec- ades of work on learning in psychology andneuroscience, one notable example being the use of reinforcement learning algorithms to pre- dict the response of dopaminergic neurons inmonkeys learning to associate a stimulus lightwith subsequent sugar reward ( 17). Although these three learning paradigms help to organize ideas, much cu rrent research involves blends across these categories. For example, semi- supervised learning makes use of unlabeled data to augment labeled data in a supervised learning",
      "section": "References",
      "page_start": 4,
      "page_end": 4,
      "chunk_index": 29
    },
    {
      "text": "blends across these categories. For example, semi- supervised learning makes use of unlabeled data to augment labeled data in a supervised learning context, and discriminative training blends ar-chitectures developed for unsupervised learning with optimization formulations that make use of labels. Model selection is the broad activity ofusing training data not only to fit a model butalso to select from a family of models, and the fact that training data do not directly indicate 258 17 JULY 2015 VOL 349 ISSUE 6245 sciencemag.org SCIENCE Topics Documents Topic proportions and assignmentsgenednagenetic.,,0.04 0.020.01 lifeevolveorganism.,,0.02 0.010.01 datanumbercomputer.,,0.02 0.020.01brainneuronnerve.,,0.04 0.020.01genes organism organismssurvive? genesgenes genetic genomeslife. computer Computer analysiscomputationalnumbers predictionsgenome sequenced Fig. 3.Topic models.",
      "section": "References",
      "page_start": 4,
      "page_end": 4,
      "chunk_index": 30
    },
    {
      "text": "organismssurvive? genesgenes genetic genomeslife. computer Computer analysiscomputationalnumbers predictionsgenome sequenced Fig. 3.Topic models. T opic modeling is a methodology for analyzing documents, where a document is viewed as a collection of words, and the words in the document are viewed as being generated by an underlying set of topics (denoted by the colors in the figure). Topics are probability distributions across words (leftmost column), and each document is characterized by a probability distribution across topics (histogram). These distributions a re inferred based on the analysis of a collection of documents and can be viewed to classify, index, and summarize the content of documents. [From ( 31). Copyright 2012, Association for Computing Machinery, Inc. Reprinted with permission]CREDIT: ISTOCK/AKINBOSTANCIARTIFICIAL INTELLIGENCE which model to use leads to the use of algo- rithms developed for bandit problems and toBayesian optimization procedures.",
      "section": "References",
      "page_start": 4,
      "page_end": 5,
      "chunk_index": 31
    },
    {
      "text": "which model to use leads to the use of algo- rithms developed for bandit problems and toBayesian optimization procedures. Active learn-ing arises when the learner is allowed to choose data points and query the trainer to request tar- geted information, such as the label of an other-wise unlabeled example. Causal modeling is the effort to go beyond simply discovering predictive relations among variables, to distinguish whichvariables causally influence others (e.g., a highwhite-blood-cell count can predict the existence of an infection, but it is the infection that causes the high white-cell count). Many issues influencethe design of learning algorithms across all ofthese paradigms, including whether data are available in batches or arrive sequentially over time, how data have been sampled, require-ments that learned models be interpretable byusers, and robustness issues that arise when data do not fit prior modeling assumptions. Emerging trends",
      "section": "References",
      "page_start": 5,
      "page_end": 5,
      "chunk_index": 32
    },
    {
      "text": "data do not fit prior modeling assumptions. Emerging trends The field of machine learn ing is sufficiently young that it is still rapidly expanding, often by invent-ing new formalizations of machine-learningproblems driven by practical applications. (An example is the development of recommendation systems, as described in Fig. 4.) One major trenddriving this expansion is a growing concern withthe environment in which a machine-learning algorithm operates. The word “environment ” here refers in part to the computing architecture;whereas a classical machine-learning system in- volved a single program running on a single ma- chine, it is now common for machine-learningsystems to be deployed in architectures that in-clude many thousands or ten of thousands of processors, such that communication constraints and issues of parallelism and distributed pro-cessing take center stage. Indeed, as depictedin Fig. 5, machine-learning systems are increas-",
      "section": "References",
      "page_start": 5,
      "page_end": 5,
      "chunk_index": 33
    },
    {
      "text": "and issues of parallelism and distributed pro-cessing take center stage. Indeed, as depictedin Fig. 5, machine-learning systems are increas- ingly taking the form of complex collections of software that run on large-scale parallel and dis-tributed computing platforms and provide a range of algorithms and services to data analysts. The word “environment ”also refers to the source of the data, which ranges from a set ofpeople who may have privacy or ownership con- cerns, to the analyst or decision-maker who may have certain requirements on a machine-learningsystem (for example, that its output be visual-izable), and to the social, legal, or political frame- work surrounding the deployment of a system. The environment also may include other machine-learning systems or other agents, and the overallcollection of systems may be cooperative or ad- versarial. Broadly speaking, environments pro- vide various resources to a learning algorithmand place constraints on those resources. Increas-",
      "section": "References",
      "page_start": 5,
      "page_end": 5,
      "chunk_index": 34
    },
    {
      "text": "versarial. Broadly speaking, environments pro- vide various resources to a learning algorithmand place constraints on those resources. Increas- ingly, machine-learning researchers are formalizing these relationships, aiming to design algorithmsthat are provably effective in various environ-ments and explicitly allow users to express and control trade-offs among resources. As an example of resource constraints, let us suppose that the data are provided by a set ofindividuals who wish to retain a degree of pri-vacy. Privacy can be formalized via the notion of “differential privacy, ”which defines a probabi- listic channel between the data and the outsideworld such that an observer of the output of the channel cannot infer reliably whether particular individuals have supplied data or not ( 18). Clas- s i c a la p p l i c a t i o n so fd i f f e r e n t i a lp r i v a c yh a v e involved insuring that queries (e.g., “what is the maximum balance across a set of accounts? ”)t o",
      "section": "References",
      "page_start": 5,
      "page_end": 5,
      "chunk_index": 35
    },
    {
      "text": "involved insuring that queries (e.g., “what is the maximum balance across a set of accounts? ”)t o a privatized database return an answer that isclose to that returned on the nonprivate data. Recent research has brought differential privacy into contact with machine learning, where que-ries involve predictions or other inferential asser-tions (e.g., “g i v e nt h ed a t aI ' v es e e ns of a r ,w h a ti s the probability that a new transaction is fraud- ulent? ”)(19,20). Placing the overall design of a privacy-enhancing machine-learning systemwithin a decision-theoretic framework provides users with a tuning knob whereby they can choose a desired level of privacy that takes into accountt h ek i n d so fq u e s t i o n st h a tw i l lb ea s k e do ft h e data and their own personal utility for the an- swers. For example, a person may be willing toreveal most of their genome in the context of",
      "section": "References",
      "page_start": 5,
      "page_end": 5,
      "chunk_index": 36
    },
    {
      "text": "data and their own personal utility for the an- swers. For example, a person may be willing toreveal most of their genome in the context of research on a disease that runs in their familybut may ask for more stringent protection if in-formation about their genome is being used to set insurance rates. Communication is anoth er resource that needs to be managed within the overall context of a distributed learning system. For example, data may be distributed across distinct physical loca-tions because their size does not allow them tobe aggregated at a single site or because of ad- ministrative boundaries. In such a setting, we may wish to impose a bit-rate communication con-straint on the machine-learning algorithm. Solvingthe design problem under such a constraint will generally show how the performance of the learn- ing system degrades under decrease in commu-nication bandwidth, but it can also reveal howthe performance improves as the number of dis-",
      "section": "References",
      "page_start": 5,
      "page_end": 5,
      "chunk_index": 37
    },
    {
      "text": "ing system degrades under decrease in commu-nication bandwidth, but it can also reveal howthe performance improves as the number of dis- t r i b u t e ds i t e s( e . g . ,m a c h i n e so rp r o c e s s o r s )i n - creases, trading off these quantities against theamount of data ( 21,22). Much as in classical in- formation theory, this line of research aims at fundamental lower bounds on achievable per-formance and specific algorithms that achievethose lower bounds. A major goal of this general line of research is to bring the kinds of statistical resources studiedin machine learning (e.g., number of data points,dimension of a parameter, and complexity of a hypothesis class) into contact with the classical computational resources of time and space. Sucha bridge is present in the “probably approximately correct ”(PAC) learning framework, which studies",
      "section": "References",
      "page_start": 5,
      "page_end": 5,
      "chunk_index": 38
    },
    {
      "text": "computational resources of time and space. Sucha bridge is present in the “probably approximately correct ”(PAC) learning framework, which studies the effect of adding a polynomial-time compu-tation constraint on this relationship among errorrates, training data size, and other parameters of the learning algorithm ( 3). Recent advances in this line of research include various lower boundsthat establish fundamental gaps in performanceachievable in certain machine-learning prob- lems (e.g., sparse regression and sparse princi- pal components analysis) via polynomial-timeand exponential-time algorithms ( 23). The core of the problem, however, involves time-data trade- offs that are far from the polynomial/exponentialboundary. The large data sets that are increas-ingly the norm require algorithms whose time and space requirements are linear or sublinear in the problem size (number of data points or num-ber of dimensions).",
      "section": "References",
      "page_start": 5,
      "page_end": 5,
      "chunk_index": 39
    },
    {
      "text": "and space requirements are linear or sublinear in the problem size (number of data points or num-ber of dimensions). Recent research focuses onm e t h o d ss u c ha ss u b s a m p l i n g ,r a n d o mp r o j e c - tions, and algorithm weakening to achieve scal- ability while retaining statistical control ( 24,25). T h eu l t i m a t eg o a li st ob ea b l et os u p p l yt i m eand space budgets to machine-learning systems in addition to accuracy requirements, with the system finding an operating point that allowssuch requirements to be realized. Opportunities and challenges Despite its practical and commercial successes, machine learning remains a young field with many underexplored research opportunities. Some of these opportunities can be seen by con-trasting current machine-learning approachesto the types of learning we observe in naturally SCIENCE sciencemag.org 17 JULY 2015 VOL 349 ISSUE 6245 259 Fig. 4. Recommendation systems. A recommen-",
      "section": "References",
      "page_start": 5,
      "page_end": 5,
      "chunk_index": 40
    },
    {
      "text": "SCIENCE sciencemag.org 17 JULY 2015 VOL 349 ISSUE 6245 259 Fig. 4. Recommendation systems. A recommen- dation system is a machine-learning system that is based on data that indicate links between a set of a users (e.g., people) and a set of items (e.g.,products). A link between a user and a product means that the user has indicated an interest in the product in some fashion (perhaps by purchas-ing that item in the past). The machine-learning prob-lem is to suggest other items to a given user that he or she may also be interested in, based on the data across all users. CREDIT: ISTOCK/CORR occurring systems such as humans and other animals, organizations, economies, and biological evolution. For example, whereas most machine- learning algorithms are targeted to learn onespecific function or data model from one singledata source, humans clearly learn many differ- ent skills and types of knowledge, from years",
      "section": "References",
      "page_start": 5,
      "page_end": 6,
      "chunk_index": 41
    },
    {
      "text": "ent skills and types of knowledge, from years of diverse training experience, supervised andunsupervised, in a simple -to-more-difficult se- quence (e.g., learning to crawl, then walk, then run). This has led some researchers to beginexploring the question of how to construct com-puter lifelong or never-ending learners that op- erate nonstop for years, learning thousands of interrelated skills or functions within an over-all architecture that allows the system to im-prove its ability to learn one skill based on having learned another ( 26–28). Another aspect of the analogy to natural learning systems sug-gests the idea of team-based, mixed-initiative learning. For example, whereas current machine- learning systems typically operate in isolationto analyze the given data, people often workin teams to collect and analyze data (e.g., biol- ogists have worked as teams to collect and an-",
      "section": "References",
      "page_start": 6,
      "page_end": 6,
      "chunk_index": 42
    },
    {
      "text": "ogists have worked as teams to collect and an- alyze genomic data, bringing together diverseexperiments and perspectives to make progresson this difficult problem). New machine-learning",
      "section": "References",
      "page_start": 6,
      "page_end": 6,
      "chunk_index": 43
    },
    {
      "text": "alyze genomic data, bringing together diverseexperiments and perspectives to make progresson this difficult problem). New machine-learning humans to jointly analyze complex data setsmight bring together the abilities of machinesto tease out subtle statistical regularities from massive data sets with the abilities of humans to draw on diverse background knowledge to gen-erate plausible explanations and suggest new hypotheses. Many theoretical results in machine learning apply to all learning systems, whetherthey are computer algorithms, animals, organ-izations, or natural evolution. As the field pro- gresses, we may see machine-learning theory and algorithms increasingly providing modelsfor understanding learning in neural systems,organizations, and biological evolution and see machine learning benef it from ongoing studies of these other types of learning systems. As with any powerful technology, machine",
      "section": "References",
      "page_start": 6,
      "page_end": 6,
      "chunk_index": 44
    },
    {
      "text": "machine learning benef it from ongoing studies of these other types of learning systems. As with any powerful technology, machine learning raises questions about which of its po-tential uses society should encourage and dis- courage. The push in recent years to collect new kinds of personal data, motivated by its eco-nomic value, leads to obvious privacy issues, as mentioned above. The increasing value of data also raises a second ethical issue: Who will haveaccess to, and ownership of, online data, and whowill reap its benefits? Currently, much data are collected by corporations for specific uses leading to improved profits, with little or no motive fordata sharing. However, the potential benefits thatsociety could realize, even from existing online data, would be considerable if those data were to be made available for public good. To illustrate, consider one simple example of how society could benefit from data that is",
      "section": "Methods",
      "page_start": 6,
      "page_end": 6,
      "chunk_index": 45
    },
    {
      "text": "be made available for public good. To illustrate, consider one simple example of how society could benefit from data that is already online today by using this data to de-crease the risk of global pandemic spread frominfectious diseases. By combining location data from online sources (e.g., location data from cell phones, from credit-card transactions at retailoutlets, and from security cameras in public placesand private buildings) with online medical data (e.g., emergency room admissions), it would be feasible today to implement a simple system totelephone individuals immediately if a personthey were in close contact with yesterday was just admitted to the emergency room with an infec- tious disease, alerting them to the symptoms theyshould watch for and precautions they should take. Here, there is clearly a tension and trade-off between personal privacy and public health, andsociety at large needs to make the decision onhow to make this trade-off. The larger point of",
      "section": "Methods",
      "page_start": 6,
      "page_end": 6,
      "chunk_index": 46
    },
    {
      "text": "between personal privacy and public health, andsociety at large needs to make the decision onhow to make this trade-off. The larger point of this example, however, is that, although the data are already online, we do not currently have thel a w s ,c u s t o m s ,c u l t u r e ,o rm e c h a n i s m st oe n a b l esociety to benefit from them, if it wishes to do so. In fact, much of these data are privately held andowned, even though they are data about each ofus. Considerations such as these suggest that machine learning is likely to be one of the most transformative technologies of the 21st century.Although it is impossible to predict the future, it appears essential that society begin now to con- sider how to maximize its benefits.",
      "section": "Methods",
      "page_start": 6,
      "page_end": 6,
      "chunk_index": 47
    },
    {
      "text": "appears essential that society begin now to con- sider how to maximize its benefits. 1. T. Hastie, R. Tibshirani, J. Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Springer, New York, 2011). 2. K. Murphy, Machine Learning: A Probabilistic Perspective (MIT Press, Cambridge, MA, 2012). 3. L. Valiant, Commun. ACM 27, 1134 –1142 (1984). 4. V. Chandrasekaran, M. I. Jordan, Proc. Natl. Acad. Sci. U.S.A. 110, E1181 –E1190 (2013). 5. S. Decatur, O. Goldreich, D. Ron, SIAM J. Comput. 29, 854 –879 (2000). 6. S. Shalev-Shwartz, O. Shamir, E. Tromer, Using more data to speed up training time, Proceedings of the Fifteenth Conference on Artificial Intelligence and Statistics , Canary Islands, Spain, 21 to 23 April, 2012. 7. S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, in Foundations and Trends in Machine Learning 3 (Now Publishers, Boston, 2011), pp. 1 –122. 8. S. Sra, S. Nowozin, S. Wright, Optimization for Machine",
      "section": "Methods",
      "page_start": 6,
      "page_end": 6,
      "chunk_index": 48
    },
    {
      "text": "Foundations and Trends in Machine Learning 3 (Now Publishers, Boston, 2011), pp. 1 –122. 8. S. Sra, S. Nowozin, S. Wright, Optimization for Machine Learning (MIT Press, Cambridge, MA, 2011). 9. J. Schmidhuber, Neural Netw. 61,8 5 –117 (2015). 10. Y. Bengio, in Foundations and Trends in Machine Learning 2 (Now Publishers, Boston, 2009), pp. 1 –127. 11. A. Krizhevsky, I. Sutskever, G. Hinton, Adv. Neural Inf. Process. Syst. 25, 1097 –1105 (2015). 12. G. Hinton et al .,IEEE Signal Process. Mag. 29,8 2 –97 (2012). 13. G. E. Hinton, R. R. Salakhutdinov, Science 313,5 0 4 –507 (2006). 14. V. Mnih et al .,Nature 518, 529 –533 (2015). 15. R. S. Sutton, A. G. Barto, Reinforcement Learning: An Introduction (MIT Press, Cambridge, MA, 1998). 16. E. Yaylali, J. S. Ivy, Partially observable MDPs (POMDPs):",
      "section": "References",
      "page_start": 6,
      "page_end": 6,
      "chunk_index": 49
    },
    {
      "text": "G. Barto, Reinforcement Learning: An Introduction (MIT Press, Cambridge, MA, 1998). 16. E. Yaylali, J. S. Ivy, Partially observable MDPs (POMDPs): and Management Science (John Wiley, New York, 2011). 17. W. Schultz, P. Dayan, P. R. Montague, Science 275, 1593 –1599 (1997). 18. C. Dwork, F. McSherry, K. Nissim, A. Smith, in Proceedings of the Third Theory of Cryptography Conference , New York, 4 to 7 March 2006, pp. 265 –284. 19. A. Blum, K. Ligett, A. Roth, J. ACM 20, (2013). 20. J. Duchi, M. I. Jordan, J. Wainwright, J. ACM 61,1–57 (2014). 21. M.-F. Balcan, A. Blum, S. Fine, Y. Mansour, Distributed learning, communication complexity and privacy. Proceedings of the 29th Conference on Computational Learning Theory , Edinburgh, UK, 26 June to 1 July 2012. 22. Y. Zhang, J. Duchi, M. Jordan, M. Wainwright, in Advances in Neural Information Processing Systems 26 , L. Bottou, C. Burges, Z. Ghahramani, M. Welling, Eds. (Curran Associates, Red Hook, NY, 2014), pp. 1 –23. 23. Q. Berthet, P.",
      "section": "References",
      "page_start": 6,
      "page_end": 6,
      "chunk_index": 50
    },
    {
      "text": "Bottou, C. Burges, Z. Ghahramani, M. Welling, Eds. (Curran Associates, Red Hook, NY, 2014), pp. 1 –23. 23. Q. Berthet, P. Rigollet, Ann. Stat. 41, 1780 –1815 (2013). 24. A. Kleiner, A. Talwalkar, P. Sarkar, M. I. Jordan, J. R. Stat. Soc., B76, 795 –816 (2014). 25. M. Mahoney, Found. Trends Machine Learn. 3, 123 –224 (2011). 26. T. Mitchell et al .,Proceedings of the Twenty-Ninth Conference on Artificial Intelligence (AAAI-15) , 25 to 30 January 2015, Austin, TX. 27. M. Taylor, P. Stone, J. Mach. Learn. Res. 10, 1633 –1685 (2009). 28. S. Thrun, L. Pratt, Learning To Learn (Kluwer Academic Press, Boston, 1998). 29. L. Wehbe et al.,PLOS ONE 9, e112575 (2014). 30. K. Xu et al .,Proceedings of the 32nd International Conference on Machine Learning , vol. 37, Lille, France, 6 to 11 July 2015, pp. 2048 –2057. 31. D. Blei, Commun. ACM 55,7 7 –84 (2012). 10.1126/science.aaa8415 260 17 JULY 2015 VOL 349 ISSUE 6245 sciencemag.org SCIENCECancer genomics, energy debugging, smart buildings",
      "section": "Introduction",
      "page_start": 6,
      "page_end": 6,
      "chunk_index": 51
    },
    {
      "text": "10.1126/science.aaa8415 260 17 JULY 2015 VOL 349 ISSUE 6245 sciencemag.org SCIENCECancer genomics, energy debugging, smart buildings Sample clean BlinkDB Spark Core Succinct T achyon Mesos Hadoop Y arnHDFS, S3, Ceph, …In-house apps Access and interfaces Processing engine Storage Resource virtualization AMPLab developed Spark Community 3rd partySparkSQLSpark streaming VeloxSparkR GraphX SplashG-OLA MLBase MLPiplines MLIib Fig. 5. Data analytics stack. Scalable machine-learning systems are layered architectures that are built on parallel and distributed computing platforms. The architecture depicted here —an open- source data analysis stack developed in the Algorithms, Machines and People (AMP) Laboratory atthe University of California, Berkeley —includes layers that interface to underlying operating systems; layers that provide distributed storage, data management, and processing; and layers that provide",
      "section": "Introduction",
      "page_start": 6,
      "page_end": 6,
      "chunk_index": 52
    },
    {
      "text": "layers that provide distributed storage, data management, and processing; and layers that provide core machine-learning competencies such as streaming, subsampling, pipelines, graph processing,and model serving. CREDIT: ISTOCK/AKINBOSTANCIARTIFICIAL INTELLIGENCE DOI: 10.1126/science.aaa8415, 255 (2015);349 ScienceM. I. Jordan and T. M. MitchellMachine learning: Trends, perspectives, and prospects This copy is for your personal, non-commercial use only. clicking here. colleagues, clients, or customers by , you can order high-quality copies for your If you wish to distribute this article to others here. following the guidelines  can be obtained by Permission to republish or repurpose articles or portions of articles ): July 18, 2015 www.sciencemag.org (this information is current as ofThe following resources related to this article are available online at",
      "section": "Introduction",
      "page_start": 6,
      "page_end": 7,
      "chunk_index": 53
    },
    {
      "text": "): July 18, 2015 www.sciencemag.org (this information is current as ofThe following resources related to this article are available online at http://www.sciencemag.org/content/349/6245/255.full.htmlversion of this article at: including high-resolution figures, can be found in the online Updated information and services, http://www.sciencemag.org/content/349/6245/255.full.html#relatedfound at:can be related to this article A list of selected additional articles on the Science Web sites http://www.sciencemag.org/content/349/6245/255.full.html#ref-list-1, 3 of which can be accessed free: cites 17 articles This article http://www.sciencemag.org/content/349/6245/255.full.html#related-urls1 articles hosted by HighWire Press; see:cited by This article has been http://www.sciencemag.org/cgi/collection/comp_mathComputers, Mathematicssubject collections: This article appears in the following",
      "section": "Introduction",
      "page_start": 7,
      "page_end": 7,
      "chunk_index": 54
    },
    {
      "text": "http://www.sciencemag.org/cgi/collection/comp_mathComputers, Mathematicssubject collections: This article appears in the following registered trademark of AAAS.  is aScience 2015 by the American Association for the Advancement of Science; all rights reserved. The title Copyright American Association for the Advancement of Science, 1200 New York Avenue NW, Washington, DC 20005. (print ISSN 0036-8075; online ISSN 1095-9203) is published weekly, except the last week in December, by theScience on July 20, 2015 www.sciencemag.org Downloaded from",
      "section": "Introduction",
      "page_start": 7,
      "page_end": 7,
      "chunk_index": 55
    }
  ]
}